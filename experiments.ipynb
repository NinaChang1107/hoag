{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "seaborn.set_style('white')\n",
    "import hoag  \n",
    "import mod_l_exp.utils as mlx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets.redivide_data:, computed partitions numbers - [0, 30000, 60000, 90000] len all 90000 DONE\n"
     ]
    }
   ],
   "source": [
    "dataset = mlx.generate_multiclass_dataset(n_samples=90000, n_features=1000, n_informative=50, n_redundant=25, n_repeated=0,\n",
    "                                      n_classes=2, n_clusters_per_class=3,\n",
    "                                      flip_y=0.1, class_sep=1.0,\n",
    "                                      random_state=1, hot_encoded=False, partitions_proportions=[1/3, 1/3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESPONSE  FUNCTION for $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting: we use logistic regression (for binary classification) and optimize regularization hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "alphas = np.linspace(-0, 20, 50)\n",
    "\n",
    "def cost_func(a):\n",
    "    clf = linear_model.LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        C=np.exp(-a), fit_intercept=True, \n",
    "        tol=1e-15, max_iter=500)\n",
    "\n",
    "    clf.fit(dataset.train.data, dataset.train.target)\n",
    "    cost = linear_model.logistic._logistic_loss(clf.coef_.ravel(), \n",
    "                                                dataset.validation.data, dataset.validation.target, 0.)\n",
    "    print('.', end='')\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................."
     ]
    }
   ],
   "source": [
    "scores = [cost_func(a) for a in alphas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArgAAAHoCAYAAACippxFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1UyLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ee10eb518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(alphas, scores)\n",
    "plt.title('minumum at ' + str(alphas[np.argmin(scores)]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the optimal $\\alpha$ is outside of default interval..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOAG - NO BOUNDS ON $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started hoag\n",
      "\n",
      "Log step 0\n",
      "-------------------  -----------\n",
      "training error       1.20545e+06\n",
      "validation error     1.21458e+06\n",
      "test error           1.22281e+06\n",
      "validation accuracy  0.5034\n",
      "test accuracy        0.5021\n",
      "alpha                0\n",
      "der alpha            0\n",
      "step size            0\n",
      "-------------------  -----------\n",
      "inner level iterations: 145, inner objective 13627.3177499, grad norm 255.0809048024641\n",
      "Inverting matrix with precision 9.975771591495612\n",
      "increased step size\n",
      "it 1, g: 14733.7494336, incr: -inf, sum lambda 1.0, epsilon: 0.0009000000000000001, L: 17.0194808665, norm grad_lambda: 17.9152430174\n",
      "\n",
      "Log step 1\n",
      "-------------------  -------------\n",
      "training error       13627.3\n",
      "validation error     14733.7\n",
      "test error           14752.7\n",
      "validation accuracy      0.772867\n",
      "test accuracy            0.775467\n",
      "alpha                    1\n",
      "der alpha              -17.9152\n",
      "step size                0.0558184\n",
      "-------------------  -------------\n",
      "inner level iterations: 0, inner objective 13642.7134656, grad norm 255.45983861202342\n",
      "Inverting matrix with precision 8.97819443234605\n",
      "it 2, g: 14733.7494336, incr: 0.0, sum lambda 3.86084584929, epsilon: 0.0008100000000000001, L: 17.0194808665, norm grad_lambda: 48.6901111941\n",
      "\n",
      "Log step 2\n",
      "-------------------  -------------\n",
      "training error       13653\n",
      "validation error     14733.7\n",
      "test error           14752.7\n",
      "validation accuracy      0.772867\n",
      "test accuracy            0.775467\n",
      "alpha                    3.86085\n",
      "der alpha              -48.6901\n",
      "step size                0.0587562\n",
      "-------------------  -------------\n",
      "inner level iterations: 0, inner objective 14044.0047608, grad norm 328.06529361755594\n",
      "Inverting matrix with precision 8.080374989111446\n",
      "it 3, g: 14733.7494336, incr: 0.0, sum lambda 53.849320881, epsilon: 0.000729, L: 17.0194808665, norm grad_lambda: 850.777894349\n",
      "\n",
      "Log step 3\n",
      "-------------------  -------------\n",
      "training error       14100.8\n",
      "validation error     14733.7\n",
      "test error           14752.7\n",
      "validation accuracy      0.772867\n",
      "test accuracy            0.775467\n",
      "alpha                   53.8493\n",
      "der alpha             -850.778\n",
      "step size                0.0587562\n",
      "-------------------  -------------\n",
      "inner level iterations: 0, inner objective 2.18156621804e+24, grad norm 1.0306961508752657e+24\n",
      "Inverting matrix with precision 7.272337490200301\n",
      "it 4, g: 14733.7494336, incr: 0.0, sum lambda 2175945.64565, epsilon: 0.0006561000000000001, L: 17.0194808665, norm grad_lambda: 37032548.7952\n",
      "\n",
      "Log step 4\n",
      "-------------------  ---------------\n",
      "training error           1.951e+07\n",
      "validation error     14733.7\n",
      "test error           14752.7\n",
      "validation accuracy      0.772867\n",
      "test accuracy            0.775467\n",
      "alpha                    2.17595e+06\n",
      "der alpha               -3.70325e+07\n",
      "step size                0.0587562\n",
      "-------------------  ---------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7d73cd9bbe57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhoag_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/luca/DATA/Progs/forked_hoag/mod_l_exp/utils.py\u001b[0m in \u001b[0;36mhoag_fit\u001b[0;34m(datasets, max_iter, alpha0, verbose, projection, do_print, hyper_step_mul)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mstp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_sup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_sup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_steps_log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/luca/DATA/Progs/forked_hoag/hoag/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, Xt, yt, Xh, yh, callback, projection, bias)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mtolerance_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolerance_decrease\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mlambda0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             verbose=self.verbose, projection=projection)\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# opt = _minimize_lbfgsb(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/luca/DATA/Progs/forked_hoag/hoag/hoag.py\u001b[0m in \u001b[0;36mhoag_lbfgs\u001b[0;34m(h_func_grad, h_hessian, h_crossed, g_func_grad, x0, bounds, lambda0, disp, maxcor, maxiter, maxiter_inner, only_fit, iprint, maxls, tolerance_decrease, callback, verbose, epsilon_tol_init, exponential_decrease_factor, projection)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# Overwrite h_func and h_grad:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mh_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_func_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambdak\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_grad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdak\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon_tol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm_init\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                     \u001b[0;31m# this one is finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luca/anaconda3/lib/python3.5/site-packages/scipy/linalg/misc.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(a, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# Differs from numpy only in non-finite handling and the use of blas.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Only use optimized norms if axis and keepdims are not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/luca/anaconda3/lib/python3.5/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1215\u001b[0;31m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[1;32m   1216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "identity = lambda x: x\n",
    "clf, res = mlx.hoag_fit(dataset, alpha0=0., projection=identity, max_iter=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without bounds the algorithm diverges... But here the step size looks kind of too big... Shouldn't it decrease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOAG WITH $\\alpha \\in [-10, 10]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started hoag\n",
      "\n",
      "Log step 0\n",
      "-------------------  -----------\n",
      "training error       1.1146e+06\n",
      "validation error     1.13231e+06\n",
      "test error           1.13028e+06\n",
      "validation accuracy  0.501167\n",
      "test accuracy        0.498267\n",
      "alpha                0\n",
      "der alpha            0\n",
      "step size            0\n",
      "-------------------  -----------\n",
      "inner level iterations: 130, inner objective 13631.4713413, grad norm 198.52092129127848\n",
      "Inverting matrix with precision 9.977236433283112\n",
      "increased step size\n",
      "it 1, g: 14729.8013085, incr: -inf, sum lambda 1.0, epsilon: 0.0009000000000000001, L: 23.2439660711, norm grad_lambda: 24.4673327064\n",
      "\n",
      "Log step 1\n",
      "-------------------  -------------\n",
      "training error       13631.5\n",
      "validation error     14729.8\n",
      "test error           14748.7\n",
      "validation accuracy      0.7724\n",
      "test accuracy            0.7753\n",
      "alpha                    1\n",
      "der alpha              -24.4673\n",
      "step size                0.0408708\n",
      "-------------------  -------------\n",
      "inner level iterations: 0, inner objective 13652.4962286, grad norm 199.02041348803093\n",
      "Inverting matrix with precision 8.979512789954802\n",
      "it 2, g: 14729.8013085, incr: 0.0, sum lambda 3.86097958024, epsilon: 0.0008100000000000001, L: 23.2439660711, norm grad_lambda: 66.5005122931\n",
      "\n",
      "Log step 2\n",
      "-------------------  -------------\n",
      "training error       13666.5\n",
      "validation error     14729.8\n",
      "test error           14748.7\n",
      "validation accuracy      0.7724\n",
      "test accuracy            0.7753\n",
      "alpha                    3.86098\n",
      "der alpha              -66.5005\n",
      "step size                0.0430219\n",
      "-------------------  -------------\n",
      "inner level iterations: 0, inner objective 14200.5903402, grad norm 309.469456624987\n",
      "Inverting matrix with precision 8.08156151095932\n",
      "it 3, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.000729, L: 23.2439660711, norm grad_lambda: 1162.18510045\n",
      "\n",
      "Log step 3\n",
      "-------------------  -------------\n",
      "training error       13741.6\n",
      "validation error     14729.8\n",
      "test error           14748.7\n",
      "validation accuracy      0.7724\n",
      "test accuracy            0.7753\n",
      "alpha                   10\n",
      "der alpha            -1162.19\n",
      "step size                0.0430219\n",
      "-------------------  -------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 7.273405359863388\n",
      "increased step size\n",
      "it 4, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.0006561000000000001, L: 22.0817677675, norm grad_lambda: 18951.1850036\n",
      "\n",
      "Log step 4\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -18951.2\n",
      "step size                 0.0430219\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 6.54606482387705\n",
      "increased step size\n",
      "it 5, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.00059049, L: 20.9776793792, norm grad_lambda: 18687.091527\n",
      "\n",
      "Log step 5\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -18687.1\n",
      "step size                 0.0452862\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 5.891458341489344\n",
      "increased step size\n",
      "it 6, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.000531441, L: 19.9287954102, norm grad_lambda: 18533.3223356\n",
      "\n",
      "Log step 6\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -18533.3\n",
      "step size                 0.0476697\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 5.30231250734041\n",
      "increased step size\n",
      "it 7, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.0004782969, L: 18.9323556397, norm grad_lambda: 18290.9530473\n",
      "\n",
      "Log step 7\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -18291\n",
      "step size                 0.0501786\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 4.772081256606369\n",
      "increased step size\n",
      "it 8, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.00043046721, L: 17.9857378577, norm grad_lambda: 18146.9484798\n",
      "\n",
      "Log step 8\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -18146.9\n",
      "step size                 0.0528196\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 4.294873130945732\n",
      "increased step size\n",
      "it 9, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.000387420489, L: 17.0864509648, norm grad_lambda: 17935.1788947\n",
      "\n",
      "Log step 9\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -17935.2\n",
      "step size                 0.0555996\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 3.865385817851159\n",
      "increased step size\n",
      "it 10, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.0003486784401, L: 16.2321284166, norm grad_lambda: 17801.5821109\n",
      "\n",
      "Log step 10\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -17801.6\n",
      "step size                 0.0585259\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 3.478847236066043\n",
      "increased step size\n",
      "it 11, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.00031381059609000004, L: 15.4205219957, norm grad_lambda: 17609.6994605\n",
      "\n",
      "Log step 11\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -17609.7\n",
      "step size                 0.0616062\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 3.130962512459439\n",
      "increased step size\n",
      "it 12, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.00028242953648100003, L: 14.649495896, norm grad_lambda: 17482.1961822\n",
      "\n",
      "Log step 12\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -17482.2\n",
      "step size                 0.0648486\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 2.8178662612134953\n",
      "increased step size\n",
      "it 13, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.00025418658283290005, L: 13.9170211012, norm grad_lambda: 17298.1347033\n",
      "\n",
      "Log step 13\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -17298.1\n",
      "step size                 0.0682617\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 2.536079635092146\n",
      "increased step size\n",
      "it 14, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.00022876792454961005, L: 13.2211700461, norm grad_lambda: 17172.8556225\n",
      "\n",
      "Log step 14\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -17172.9\n",
      "step size                 0.0718545\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 2.282471671582931\n",
      "increased step size\n",
      "it 15, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.00020589113209464906, L: 12.5601115438, norm grad_lambda: 13166.3419652\n",
      "\n",
      "Log step 15\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -13166.3\n",
      "step size                 0.0756363\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 2.054224504424638\n",
      "increased step size\n",
      "it 16, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.00018530201888518417, L: 11.9321059666, norm grad_lambda: 13059.6964294\n",
      "\n",
      "Log step 16\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -13059.7\n",
      "step size                 0.0796171\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 1.8488020539821746\n",
      "increased step size\n",
      "it 17, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.00016677181699666576, L: 11.3355006683, norm grad_lambda: 12784.1816548\n",
      "\n",
      "Log step 17\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -12784.2\n",
      "step size                 0.0838075\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 1.6639218485839573\n",
      "increased step size\n",
      "it 18, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.0001500946352969992, L: 10.7687256349, norm grad_lambda: 12680.9702747\n",
      "\n",
      "Log step 18\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -12681\n",
      "step size                 0.0882184\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 1.4975296637255617\n",
      "increased step size\n",
      "it 19, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.0001350851717672993, L: 10.2302893531, norm grad_lambda: 12418.7042646\n",
      "\n",
      "Log step 19\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -12418.7\n",
      "step size                 0.0928615\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 1.3477766973530056\n",
      "increased step size\n",
      "it 20, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.00012157665459056936, L: 9.71877488546, norm grad_lambda: 12320.5197502\n",
      "\n",
      "Log step 20\n",
      "-------------------  --------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -12320.5\n",
      "step size                 0.0977489\n",
      "-------------------  --------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 1.212999027617705\n",
      "increased step size\n",
      "it 21, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 0.00010941898913151243, L: 9.23283614119, norm grad_lambda: 12082.3910714\n",
      "\n",
      "Log step 21\n",
      "-------------------  -------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -12082.4\n",
      "step size                 0.102894\n",
      "-------------------  -------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 1.0916991248559347\n",
      "increased step size\n",
      "it 22, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 9.847709021836118e-05, L: 8.77119433413, norm grad_lambda: 11989.9665903\n",
      "\n",
      "Log step 22\n",
      "-------------------  -------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -11990\n",
      "step size                 0.108309\n",
      "-------------------  -------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.9825292123703411\n",
      "increased step size\n",
      "it 23, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 8.862938119652506e-05, L: 8.33263461742, norm grad_lambda: 11772.9689364\n",
      "\n",
      "Log step 23\n",
      "-------------------  ------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -11773\n",
      "step size                 0.11401\n",
      "-------------------  ------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.884276291133307\n",
      "increased step size\n",
      "it 24, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 7.976644307687256e-05, L: 7.91600288655, norm grad_lambda: 11684.7637605\n",
      "\n",
      "Log step 24\n",
      "-------------------  ------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -11684.8\n",
      "step size                 0.12001\n",
      "-------------------  ------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.7958486620199763\n",
      "increased step size\n",
      "it 25, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 7.17897987691853e-05, L: 7.52020274223, norm grad_lambda: 11478.5821881\n",
      "\n",
      "Log step 25\n",
      "-------------------  -------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -11478.6\n",
      "step size                 0.126326\n",
      "-------------------  -------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.7162637958179786\n",
      "increased step size\n",
      "it 26, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 6.461081889226677e-05, L: 7.14419260511, norm grad_lambda: 11393.2232271\n",
      "\n",
      "Log step 26\n",
      "-------------------  -------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -11393.2\n",
      "step size                 0.132975\n",
      "-------------------  -------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.6446374162361808\n",
      "increased step size\n",
      "it 27, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 5.81497370030401e-05, L: 6.78698297486, norm grad_lambda: 11193.4376203\n",
      "\n",
      "Log step 27\n",
      "-------------------  -------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -11193.4\n",
      "step size                 0.139974\n",
      "-------------------  -------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.5801736746125628\n",
      "increased step size\n",
      "it 28, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 5.233476330273609e-05, L: 6.44763382612, norm grad_lambda: 11110.4486154\n",
      "\n",
      "Log step 28\n",
      "-------------------  -------------\n",
      "training error        13741.6\n",
      "validation error      14729.8\n",
      "test error            14748.7\n",
      "validation accuracy       0.7724\n",
      "test accuracy             0.7753\n",
      "alpha                    10\n",
      "der alpha            -11110.4\n",
      "step size                 0.147341\n",
      "-------------------  -------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.5221563071513065\n",
      "increased step size\n",
      "it 29, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 4.7101286972462485e-05, L: 6.12525213481, norm grad_lambda: 6795.91663422\n",
      "\n",
      "Log step 29\n",
      "-------------------  ------------\n",
      "training error       13741.6\n",
      "validation error     14729.8\n",
      "test error           14748.7\n",
      "validation accuracy      0.7724\n",
      "test accuracy            0.7753\n",
      "alpha                   10\n",
      "der alpha            -6795.92\n",
      "step size                0.155096\n",
      "-------------------  ------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.4699406764361759\n",
      "increased step size\n",
      "it 30, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 4.239115827521624e-05, L: 5.81898952807, norm grad_lambda: 5812.77112214\n",
      "\n",
      "Log step 30\n",
      "-------------------  ------------\n",
      "training error       13741.6\n",
      "validation error     14729.8\n",
      "test error           14748.7\n",
      "validation accuracy      0.7724\n",
      "test accuracy            0.7753\n",
      "alpha                   10\n",
      "der alpha            -5812.77\n",
      "step size                0.163259\n",
      "-------------------  ------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.4229466087925583\n",
      "increased step size\n",
      "it 31, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 3.8152042447694614e-05, L: 5.52804005167, norm grad_lambda: 5474.34633568\n",
      "\n",
      "Log step 31\n",
      "-------------------  ------------\n",
      "training error       13741.6\n",
      "validation error     14729.8\n",
      "test error           14748.7\n",
      "validation accuracy      0.7724\n",
      "test accuracy            0.7753\n",
      "alpha                   10\n",
      "der alpha            -5474.35\n",
      "step size                0.171851\n",
      "-------------------  ------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.3806519479133025\n",
      "increased step size\n",
      "it 32, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 3.433683820292515e-05, L: 5.25163804908, norm grad_lambda: 5444.56847413\n",
      "\n",
      "Log step 32\n",
      "-------------------  ------------\n",
      "training error       13741.6\n",
      "validation error     14729.8\n",
      "test error           14748.7\n",
      "validation accuracy      0.7724\n",
      "test accuracy            0.7753\n",
      "alpha                   10\n",
      "der alpha            -5444.57\n",
      "step size                0.180896\n",
      "-------------------  ------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.3425867531219722\n",
      "increased step size\n",
      "it 33, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 3.090315438263264e-05, L: 4.98905614663, norm grad_lambda: 5221.8652426\n",
      "\n",
      "Log step 33\n",
      "-------------------  ------------\n",
      "training error       13741.6\n",
      "validation error     14729.8\n",
      "test error           14748.7\n",
      "validation accuracy      0.7724\n",
      "test accuracy            0.7753\n",
      "alpha                   10\n",
      "der alpha            -5221.87\n",
      "step size                0.190417\n",
      "-------------------  ------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.308328077809775\n",
      "increased step size\n",
      "it 34, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 2.7812838944369376e-05, L: 4.7396033393, norm grad_lambda: 5193.32946179\n",
      "\n",
      "Log step 34\n",
      "-------------------  ------------\n",
      "training error       13741.6\n",
      "validation error     14729.8\n",
      "test error           14748.7\n",
      "validation accuracy      0.7724\n",
      "test accuracy            0.7753\n",
      "alpha                   10\n",
      "der alpha            -5193.33\n",
      "step size                0.200439\n",
      "-------------------  ------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.27749527002879754\n",
      "increased step size\n",
      "it 35, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 2.503155504993244e-05, L: 4.50262317233, norm grad_lambda: 4986.33326844\n",
      "\n",
      "Log step 35\n",
      "-------------------  ------------\n",
      "training error       13741.6\n",
      "validation error     14729.8\n",
      "test error           14748.7\n",
      "validation accuracy      0.7724\n",
      "test accuracy            0.7753\n",
      "alpha                   10\n",
      "der alpha            -4986.33\n",
      "step size                0.210988\n",
      "-------------------  ------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.2497457430259178\n",
      "increased step size\n",
      "it 36, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 2.2528399544939195e-05, L: 4.27749201372, norm grad_lambda: 4959.34963447\n",
      "\n",
      "Log step 36\n",
      "-------------------  ------------\n",
      "training error       13741.6\n",
      "validation error     14729.8\n",
      "test error           14748.7\n",
      "validation accuracy      0.7724\n",
      "test accuracy            0.7753\n",
      "alpha                   10\n",
      "der alpha            -4959.35\n",
      "step size                0.222093\n",
      "-------------------  ------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.224771168723326\n",
      "increased step size\n",
      "it 37, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 2.0275559590445276e-05, L: 4.06361741303, norm grad_lambda: 1167.00989863\n",
      "\n",
      "Log step 37\n",
      "-------------------  ------------\n",
      "training error       13741.6\n",
      "validation error     14729.8\n",
      "test error           14748.7\n",
      "validation accuracy      0.7724\n",
      "test accuracy            0.7753\n",
      "alpha                   10\n",
      "der alpha            -1167.01\n",
      "step size                0.233782\n",
      "-------------------  ------------\n",
      "inner level iterations: 0, inner objective 283134.954457, grad norm 108965.76553461057\n",
      "Inverting matrix with precision 0.2022940518509934\n",
      "increased step size\n",
      "it 38, g: 14729.8013085, incr: 0.0, sum lambda 10.0, epsilon: 1.8248003631400748e-05, L: 3.86043654238, norm grad_lambda: 1146.76673082\n",
      "\n",
      "Log step 38\n",
      "-------------------  ------------\n",
      "training error       13741.6\n",
      "validation error     14729.8\n",
      "test error           14748.7\n",
      "validation accuracy      0.7724\n",
      "test accuracy            0.7753\n",
      "alpha                   10\n",
      "der alpha            -1146.77\n",
      "step size                0.246086\n",
      "-------------------  ------------\n",
      "inner level iterations: 0, inner objective 185356.387607, grad norm 87323.14595814401\n",
      "Inverting matrix with precision 0.26670403716335056\n",
      "increased step size\n",
      "it 39, g: 14683.7781787, incr: -46.0231297492, sum lambda 10.0, epsilon: 1.6423203268260675e-05, L: 3.66741471526, norm grad_lambda: 632.589602756\n",
      "\n",
      "Log step 39\n",
      "-------------------  ------------\n",
      "training error       13868.2\n",
      "validation error     14683.8\n",
      "test error           14687.9\n",
      "validation accuracy      0.7721\n",
      "test accuracy            0.775667\n",
      "alpha                   10\n",
      "der alpha             -632.59\n",
      "step size                0.259038\n",
      "-------------------  ------------\n",
      "inner level iterations: 0, inner objective 185356.387607, grad norm 87323.14595814401\n",
      "Inverting matrix with precision 0.24003363344701553\n",
      "increased step size\n",
      "it 40, g: 14683.7781787, incr: 0.0, sum lambda 10.0, epsilon: 1.4780882941434607e-05, L: 3.4840439795, norm grad_lambda: 627.282960367\n",
      "\n",
      "Log step 40\n",
      "-------------------  ------------\n",
      "training error       13868.2\n",
      "validation error     14683.8\n",
      "test error           14687.9\n",
      "validation accuracy      0.7721\n",
      "test accuracy            0.775667\n",
      "alpha                   10\n",
      "der alpha             -627.283\n",
      "step size                0.272672\n",
      "-------------------  ------------\n",
      "inner level iterations: 3, inner objective 19496.0541812, grad norm 63348.273612360055\n",
      "Inverting matrix with precision 0.8932353605387646\n",
      "decrease step size\n",
      "!!step size rejected!! 18639.2298198 14683.7781787\n",
      "it 41, g: 18639.2298198, incr: 0.0, sum lambda 10.0, epsilon: 6.651397323645573e-06, L: 6.96808795899, norm grad_lambda: 641.844441072\n",
      "\n",
      "Log step 41\n",
      "-------------------  ------------\n",
      "training error       18462.7\n",
      "validation error     18639.2\n",
      "test error           18427.9\n",
      "validation accuracy      0.730967\n",
      "test accuracy            0.737567\n",
      "alpha                   10\n",
      "der alpha             -641.844\n",
      "step size                0.287023\n",
      "-------------------  ------------\n",
      "inner level iterations: 3, inner objective 15244.2920047, grad norm 19584.081229689564\n",
      "Inverting matrix with precision 0.13575888695323346\n",
      "increased step size\n",
      "it 42, g: 14786.4897432, incr: -3852.74007654, sum lambda -10.0, epsilon: 5.986257591281016e-06, L: 6.61968356104, norm grad_lambda: 290.539627102\n",
      "\n",
      "Log step 42\n",
      "-------------------  ------------\n",
      "training error       14643.4\n",
      "validation error     14786.5\n",
      "test error           14742.7\n",
      "validation accuracy      0.774467\n",
      "test accuracy            0.776667\n",
      "alpha                  -10\n",
      "der alpha              290.54\n",
      "step size                0.143511\n",
      "-------------------  ------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 265, inner objective 13617.8411484, grad norm 0.001257424720360627\n",
      "Inverting matrix with precision 0.05950343115436721\n",
      "increased step size\n",
      "it 43, g: 14716.8961609, incr: -69.5935823132, sum lambda -9.99999810582, epsilon: 5.387631832152914e-06, L: 6.28869938299, norm grad_lambda: 1.25388887455e-05\n",
      "\n",
      "Log step 43\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                  -10\n",
      "der alpha               -1.25389e-05\n",
      "step size                0.151065\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0008821106576822915\n",
      "Inverting matrix with precision 0.053553087272847674\n",
      "increased step size\n",
      "it 44, g: 14716.8961605, incr: -4.12457666243e-07, sum lambda -9.99999611128, epsilon: 4.848868648937623e-06, L: 5.97426441384, norm grad_lambda: 1.25430590596e-05\n",
      "\n",
      "Log step 44\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                  -10\n",
      "der alpha               -1.25431e-05\n",
      "step size                0.159015\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0009400148041131297\n",
      "Inverting matrix with precision 0.048197779739187804\n",
      "increased step size\n",
      "it 45, g: 14716.896161, incr: 4.48375431006e-07, sum lambda -9.99999401186, epsilon: 4.363981784043861e-06, L: 5.67555119315, norm grad_lambda: 1.25424711731e-05\n",
      "\n",
      "Log step 45\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99999\n",
      "der alpha               -1.25425e-05\n",
      "step size                0.167385\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0007322526322560678\n",
      "Inverting matrix with precision 0.04337800069172404\n",
      "increased step size\n",
      "it 46, g: 14716.8961602, incr: -7.91036654846e-07, sum lambda -9.99999180181, epsilon: 3.927583605639475e-06, L: 5.39177363349, norm grad_lambda: 1.25432555827e-05\n",
      "\n",
      "Log step 46\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99999\n",
      "der alpha               -1.25433e-05\n",
      "step size                0.176194\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0008015868020492153\n",
      "Inverting matrix with precision 0.03904020147279474\n",
      "increased step size\n",
      "it 47, g: 14716.8961606, incr: 4.63978722109e-07, sum lambda -9.99998947543, epsilon: 3.534825245075528e-06, L: 5.12218495182, norm grad_lambda: 1.25433363582e-05\n",
      "\n",
      "Log step 47\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99999\n",
      "der alpha               -1.25433e-05\n",
      "step size                0.185468\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0006399546837305947\n",
      "Inverting matrix with precision 0.03513618058030347\n",
      "increased step size\n",
      "it 48, g: 14716.89616, incr: -6.36786353425e-07, sum lambda -9.99998702656, epsilon: 3.1813427205679753e-06, L: 4.86607570423, norm grad_lambda: 1.25435644804e-05\n",
      "\n",
      "Log step 48\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99999\n",
      "der alpha               -1.25436e-05\n",
      "step size                0.195229\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 2, inner objective 13617.8411484, grad norm 0.0005712363900786517\n",
      "Inverting matrix with precision 0.03162256301855416\n",
      "increased step size\n",
      "it 49, g: 14716.8961602, incr: 1.65358869708e-07, sum lambda -9.99998444878, epsilon: 2.8632084485111777e-06, L: 4.62277191902, norm grad_lambda: 1.25436562852e-05\n",
      "\n",
      "Log step 49\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99998\n",
      "der alpha               -1.25437e-05\n",
      "step size                0.205504\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 2, inner objective 13617.8411484, grad norm 0.0005137468448262735\n",
      "Inverting matrix with precision 0.028460306271321918\n",
      "increased step size\n",
      "it 50, g: 14716.8961598, incr: -3.79432094633e-07, sum lambda -9.99998173438, epsilon: 2.57688760366006e-06, L: 4.39163332306, norm grad_lambda: 1.25480618311e-05\n",
      "\n",
      "Log step 50\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99998\n",
      "der alpha               -1.25481e-05\n",
      "step size                0.21632\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0005505691981328231\n",
      "Inverting matrix with precision 0.02561427603664519\n",
      "increased step size\n",
      "it 51, g: 14716.8961603, incr: 4.66379788122e-07, sum lambda -9.99997887706, epsilon: 2.319198843294054e-06, L: 4.17205165691, norm grad_lambda: 1.25482946901e-05\n",
      "\n",
      "Log step 51\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99998\n",
      "der alpha               -1.25483e-05\n",
      "step size                0.227706\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0005505582260596073\n",
      "Inverting matrix with precision 0.023052848432973765\n",
      "increased step size\n",
      "it 52, g: 14716.8961603, incr: -5.45696821064e-12, sum lambda -9.99997586933, epsilon: 2.087278958964649e-06, L: 3.96344907407, norm grad_lambda: 1.25483989623e-05\n",
      "\n",
      "Log step 52\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99998\n",
      "der alpha               -1.25484e-05\n",
      "step size                0.23969\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.000550514862568496\n",
      "Inverting matrix with precision 0.020747563589651786\n",
      "increased step size\n",
      "it 53, g: 14716.8961603, incr: -2.91038304567e-11, sum lambda -9.99997270328, epsilon: 1.8785510630681842e-06, L: 3.76527662036, norm grad_lambda: 1.2548473952e-05\n",
      "\n",
      "Log step 53\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99997\n",
      "der alpha               -1.25485e-05\n",
      "step size                0.252305\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'ABNORMAL_TERMINATION_IN_LNSRCH                             \\x00'\n",
      "inner level iterations: 0, inner objective 13617.8411484, grad norm 0.0005505148610472886\n",
      "Inverting matrix with precision 0.01867280723068661\n",
      "increased step size\n",
      "it 54, g: 14716.8961603, incr: 0.0, sum lambda -9.99996937057, epsilon: 1.6906959567613658e-06, L: 3.57701278934, norm grad_lambda: 1.25485733394e-05\n",
      "\n",
      "Log step 54\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99997\n",
      "der alpha               -1.25486e-05\n",
      "step size                0.265585\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'ABNORMAL_TERMINATION_IN_LNSRCH                             \\x00'\n",
      "inner level iterations: 0, inner objective 13617.8411484, grad norm 0.0005505148594460139\n",
      "Inverting matrix with precision 0.01680552650761795\n",
      "increased step size\n",
      "it 55, g: 14716.8961603, incr: 0.0, sum lambda -9.99996586243, epsilon: 1.5216263610852293e-06, L: 3.39816214988, norm grad_lambda: 1.25486532509e-05\n",
      "\n",
      "Log step 55\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99997\n",
      "der alpha               -1.25487e-05\n",
      "step size                0.279563\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'ABNORMAL_TERMINATION_IN_LNSRCH                             \\x00'\n",
      "inner level iterations: 0, inner objective 13617.8411484, grad norm 0.0005505148577604609\n",
      "Inverting matrix with precision 0.015124973856856155\n",
      "increased step size\n",
      "it 56, g: 14716.8961603, incr: 0.0, sum lambda -9.99996216963, epsilon: 1.3694637249767064e-06, L: 3.22825404238, norm grad_lambda: 1.25487502142e-05\n",
      "\n",
      "Log step 56\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99996\n",
      "der alpha               -1.25488e-05\n",
      "step size                0.294277\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'ABNORMAL_TERMINATION_IN_LNSRCH                             \\x00'\n",
      "inner level iterations: 0, inner objective 13617.8411484, grad norm 0.000550514855986192\n",
      "Inverting matrix with precision 0.01361247647117054\n",
      "increased step size\n",
      "it 57, g: 14716.8961603, incr: 0.0, sum lambda -9.99995828244, epsilon: 1.2325173524790358e-06, L: 3.06684134026, norm grad_lambda: 1.25488345931e-05\n",
      "\n",
      "Log step 57\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99996\n",
      "der alpha               -1.25488e-05\n",
      "step size                0.309765\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'ABNORMAL_TERMINATION_IN_LNSRCH                             \\x00'\n",
      "inner level iterations: 0, inner objective 13617.8411484, grad norm 0.0005505148541185401\n",
      "Inverting matrix with precision 0.012251228824053486\n",
      "increased step size\n",
      "it 58, g: 14716.8961603, incr: 0.0, sum lambda -9.99995419063, epsilon: 1.1092656172311322e-06, L: 2.91349927325, norm grad_lambda: 1.25489326064e-05\n",
      "\n",
      "Log step 58\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99995\n",
      "der alpha               -1.25489e-05\n",
      "step size                0.326068\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'ABNORMAL_TERMINATION_IN_LNSRCH                             \\x00'\n",
      "inner level iterations: 0, inner objective 13617.8411484, grad norm 0.000550514852152589\n",
      "Inverting matrix with precision 0.011026105941648138\n",
      "increased step size\n",
      "it 59, g: 14716.8961603, incr: 0.0, sum lambda -9.99994988343, epsilon: 9.98339055508019e-07, L: 2.76782430959, norm grad_lambda: 1.25490225016e-05\n",
      "\n",
      "Log step 59\n",
      "-------------------  --------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99995\n",
      "der alpha               -1.2549e-05\n",
      "step size                0.34323\n",
      "-------------------  --------------\n",
      "LBFGS decided finish!\n",
      "b'ABNORMAL_TERMINATION_IN_LNSRCH                             \\x00'\n",
      "inner level iterations: 0, inner objective 13617.8411484, grad norm 0.000550514850083167\n",
      "Inverting matrix with precision 0.009923495347483323\n",
      "increased step size\n",
      "it 60, g: 14716.8961603, incr: 0.0, sum lambda -9.9999453495, epsilon: 8.985051499572171e-07, L: 2.62943309411, norm grad_lambda: 1.25491242964e-05\n",
      "\n",
      "Log step 60\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99995\n",
      "der alpha               -1.25491e-05\n",
      "step size                0.361295\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'ABNORMAL_TERMINATION_IN_LNSRCH                             \\x00'\n",
      "inner level iterations: 0, inner objective 13617.8411484, grad norm 0.0005505148479048272\n",
      "Inverting matrix with precision 0.008931145812734992\n",
      "increased step size\n",
      "it 61, g: 14716.8961603, incr: 0.0, sum lambda -9.9999405769, epsilon: 8.086546349614954e-07, L: 2.4979614394, norm grad_lambda: 1.25492202629e-05\n",
      "\n",
      "Log step 61\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99994\n",
      "der alpha               -1.25492e-05\n",
      "step size                0.38031\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'ABNORMAL_TERMINATION_IN_LNSRCH                             \\x00'\n",
      "inner level iterations: 0, inner objective 13617.8411484, grad norm 0.0005505148456118388\n",
      "Inverting matrix with precision 0.008038031231461493\n",
      "increased step size\n",
      "it 62, g: 14716.8961603, incr: 0.0, sum lambda -9.99993554433, epsilon: 7.277891714653459e-07, L: 2.37306336743, norm grad_lambda: 1.25711710737e-05\n",
      "\n",
      "Log step 62\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99994\n",
      "der alpha               -1.25712e-05\n",
      "step size                0.400326\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'ABNORMAL_TERMINATION_IN_LNSRCH                             \\x00'\n",
      "inner level iterations: 0, inner objective 13617.8411484, grad norm 0.0005505148431939655\n",
      "Inverting matrix with precision 0.007234228108315343\n",
      "increased step size\n",
      "it 63, g: 14716.8961603, incr: 0.0, sum lambda -9.99993024683, epsilon: 6.550102543188112e-07, L: 2.25441019906, norm grad_lambda: 1.25713022426e-05\n",
      "\n",
      "Log step 63\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99993\n",
      "der alpha               -1.25713e-05\n",
      "step size                0.421396\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0007666742286939203\n",
      "Inverting matrix with precision 0.006510805154726748\n",
      "increased step size\n",
      "it 64, g: 14716.8961597, incr: -5.22357368027e-07, sum lambda -9.99992467047, epsilon: 5.895092288869302e-07, L: 2.14168968911, norm grad_lambda: 1.25714041306e-05\n",
      "\n",
      "Log step 64\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99992\n",
      "der alpha               -1.25714e-05\n",
      "step size                0.443575\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'ABNORMAL_TERMINATION_IN_LNSRCH                             \\x00'\n",
      "inner level iterations: 0, inner objective 13617.8411484, grad norm 0.0007666742269042055\n",
      "Inverting matrix with precision 0.0058597246392540735\n",
      "increased step size\n",
      "it 65, g: 14716.8961597, incr: 0.0, sum lambda -9.99991880056, epsilon: 5.305583059982371e-07, L: 2.03460520465, norm grad_lambda: 1.25715351797e-05\n",
      "\n",
      "Log step 65\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99992\n",
      "der alpha               -1.25715e-05\n",
      "step size                0.466921\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0017365511026967554\n",
      "Inverting matrix with precision 0.005273752429660813\n",
      "it 66, g: 14716.8961611, incr: 1.39027361001e-06, sum lambda -9.99991262165, epsilon: 4.775024753984134e-07, L: 2.03460520465, norm grad_lambda: 1.25716429831e-05\n",
      "\n",
      "Log step 66\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99991\n",
      "der alpha               -1.25716e-05\n",
      "step size                0.491496\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.001074363448693342\n",
      "Inverting matrix with precision 0.004746377120696031\n",
      "increased step size\n",
      "it 67, g: 14716.8961608, incr: -3.62984792446e-07, sum lambda -9.99990644267, epsilon: 4.297522278585721e-07, L: 1.93287494442, norm grad_lambda: 1.2571779558e-05\n",
      "\n",
      "Log step 67\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99991\n",
      "der alpha               -1.25718e-05\n",
      "step size                0.491496\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0010743631753652827\n",
      "Inverting matrix with precision 0.004271739408626401\n",
      "increased step size\n",
      "it 68, g: 14716.8961608, incr: -1.81898940355e-12, sum lambda -9.99989993843, epsilon: 3.867770050727149e-07, L: 1.8362311972, norm grad_lambda: 1.25718919628e-05\n",
      "\n",
      "Log step 68\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.9999\n",
      "der alpha               -1.25719e-05\n",
      "step size                0.517364\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0003719780303881232\n",
      "Inverting matrix with precision 0.0038445653791111417\n",
      "increased step size\n",
      "it 69, g: 14716.8961602, incr: -5.89658156969e-07, sum lambda -9.99989309177, epsilon: 3.480993045654434e-07, L: 1.74441963734, norm grad_lambda: 1.25720331075e-05\n",
      "\n",
      "Log step 69\n",
      "-------------------  --------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99989\n",
      "der alpha               -1.2572e-05\n",
      "step size                0.544594\n",
      "-------------------  --------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0003719696687815478\n",
      "Inverting matrix with precision 0.003460108841201721\n",
      "increased step size\n",
      "it 70, g: 14716.8961602, incr: 1.81898940355e-11, sum lambda -9.9998858847, epsilon: 3.132893741088991e-07, L: 1.65719865547, norm grad_lambda: 1.25721554791e-05\n",
      "\n",
      "Log step 70\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99989\n",
      "der alpha               -1.25722e-05\n",
      "step size                0.573257\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0013405479712196356\n",
      "Inverting matrix with precision 0.003114098047139785\n",
      "it 71, g: 14716.8961613, incr: 1.09447682917e-06, sum lambda -9.99987829822, epsilon: 2.8196043669800916e-07, L: 1.65719865547, norm grad_lambda: 1.25723066545e-05\n",
      "\n",
      "Log step 71\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99988\n",
      "der alpha               -1.25723e-05\n",
      "step size                0.603428\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0013384166192685785\n",
      "Inverting matrix with precision 0.0028026882423010458\n",
      "increased step size\n",
      "it 72, g: 14716.8961613, incr: -1.14596332423e-09, sum lambda -9.99987071166, epsilon: 2.5376439302820823e-07, L: 1.5743387227, norm grad_lambda: 1.25724400684e-05\n",
      "\n",
      "Log step 72\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99987\n",
      "der alpha               -1.25724e-05\n",
      "step size                0.603428\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0013384144095848335\n",
      "Inverting matrix with precision 0.0025224194180708255\n",
      "increased step size\n",
      "it 73, g: 14716.8961613, incr: -1.81898940355e-12, sum lambda -9.9998627245, epsilon: 2.283879537253874e-07, L: 1.49562178656, norm grad_lambda: 1.25744993803e-05\n",
      "\n",
      "Log step 73\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99986\n",
      "der alpha               -1.25745e-05\n",
      "step size                0.635187\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0013274916652198524\n",
      "Inverting matrix with precision 0.0022701774757456954\n",
      "increased step size\n",
      "it 74, g: 14716.8961613, incr: -5.88079274166e-09, sum lambda -9.99985431592, epsilon: 2.0554915835284867e-07, L: 1.42084069724, norm grad_lambda: 1.25760561451e-05\n",
      "\n",
      "Log step 74\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99985\n",
      "der alpha               -1.25761e-05\n",
      "step size                0.668618\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0007390625459760753\n",
      "Inverting matrix with precision 0.0020431597023607595\n",
      "increased step size\n",
      "it 75, g: 14716.8961609, incr: -3.25424480252e-07, sum lambda -9.99984546441, epsilon: 1.849942425175638e-07, L: 1.34979866237, norm grad_lambda: 1.25765762815e-05\n",
      "\n",
      "Log step 75\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99985\n",
      "der alpha               -1.25766e-05\n",
      "step size                0.703809\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0006415512574940488\n",
      "Inverting matrix with precision 0.001838843728004465\n",
      "increased step size\n",
      "it 76, g: 14716.8961609, incr: -5.54082362214e-08, sum lambda -9.99983614693, epsilon: 1.6649481826580745e-07, L: 1.28230872926, norm grad_lambda: 1.25767230542e-05\n",
      "\n",
      "Log step 76\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99984\n",
      "der alpha               -1.25767e-05\n",
      "step size                0.740851\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0003776571114177819\n",
      "Inverting matrix with precision 0.0016549593302717085\n",
      "increased step size\n",
      "it 77, g: 14716.8961605, incr: -3.66137101082e-07, sum lambda -9.99982633894, epsilon: 1.4984533643922672e-07, L: 1.21819329279, norm grad_lambda: 1.257688239e-05\n",
      "\n",
      "Log step 77\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99983\n",
      "der alpha               -1.25769e-05\n",
      "step size                0.779843\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0011437295416371171\n",
      "Inverting matrix with precision 0.0014894634363763714\n",
      "it 78, g: 14716.8961614, incr: 8.9575041784e-07, sum lambda -9.9998160146, epsilon: 1.3486080279530404e-07, L: 1.21819329279, norm grad_lambda: 1.25770407939e-05\n",
      "\n",
      "Log step 78\n",
      "-------------------  --------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99982\n",
      "der alpha               -1.2577e-05\n",
      "step size                0.820888\n",
      "-------------------  --------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0009605828032147312\n",
      "Inverting matrix with precision 0.001340517087517068\n",
      "increased step size\n",
      "it 79, g: 14716.8961613, incr: -9.99170879368e-08, sum lambda -9.99980567627, epsilon: 1.2137472251577365e-07, L: 1.15728362815, norm grad_lambda: 1.25940803487e-05\n",
      "\n",
      "Log step 79\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99981\n",
      "der alpha               -1.25941e-05\n",
      "step size                0.820888\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0020994300900979033\n",
      "Inverting matrix with precision 0.0012064653057662523\n",
      "increased step size\n",
      "it 80, g: 14716.8961598, incr: -1.53641667566e-06, sum lambda -9.9997947937, epsilon: 1.0923725026419628e-07, L: 1.09941944675, norm grad_lambda: 1.25942224467e-05\n",
      "\n",
      "Log step 80\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99979\n",
      "der alpha               -1.25942e-05\n",
      "step size                0.864092\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0009491254329686881\n",
      "Inverting matrix with precision 0.0010858188011863379\n",
      "it 81, g: 14716.8961604, incr: 6.5869789978e-07, sum lambda -9.99978333823, epsilon: 9.831352523777665e-08, L: 1.09941944675, norm grad_lambda: 1.25943637155e-05\n",
      "\n",
      "Log step 81\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99978\n",
      "der alpha               -1.25944e-05\n",
      "step size                0.909571\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.00032885950790941405\n",
      "Inverting matrix with precision 0.0009772369409947275\n",
      "it 82, g: 14716.896161, incr: 5.77936589252e-07, sum lambda -9.99977188262, epsilon: 8.848217271399898e-08, L: 1.09941944675, norm grad_lambda: 1.25945163618e-05\n",
      "\n",
      "Log step 82\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99977\n",
      "der alpha               -1.25945e-05\n",
      "step size                0.909571\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0012344386083421107\n",
      "Inverting matrix with precision 0.0008795132236573797\n",
      "increased step size\n",
      "it 83, g: 14716.8961606, incr: -3.89760316466e-07, sum lambda -9.99976042688, epsilon: 7.963395544259909e-08, L: 1.04444847441, norm grad_lambda: 1.25946639744e-05\n",
      "\n",
      "Log step 83\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99976\n",
      "der alpha               -1.25947e-05\n",
      "step size                0.909571\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0003076978564727085\n",
      "Inverting matrix with precision 0.0007915619217382846\n",
      "it 84, g: 14716.8961613, incr: 7.20641764929e-07, sum lambda -9.99974836806, epsilon: 7.167055989833918e-08, L: 1.04444847441, norm grad_lambda: 1.25948145854e-05\n",
      "\n",
      "Log step 84\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99975\n",
      "der alpha               -1.25948e-05\n",
      "step size                0.957443\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.000913099183896606\n",
      "Inverting matrix with precision 0.0007124057160087905\n",
      "increased step size\n",
      "it 85, g: 14716.8961611, incr: -2.01976945391e-07, sum lambda -9.9997363091, epsilon: 6.450350390850527e-08, L: 0.992226050688, norm grad_lambda: 1.25949693665e-05\n",
      "\n",
      "Log step 85\n",
      "-------------------  --------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99974\n",
      "der alpha               -1.2595e-05\n",
      "step size                0.957443\n",
      "-------------------  --------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0003049038255233084\n",
      "Inverting matrix with precision 0.0006411651569319883\n",
      "it 86, g: 14716.8961617, incr: 5.50242475583e-07, sum lambda -9.99972361508, epsilon: 5.8053153517654746e-08, L: 0.992226050688, norm grad_lambda: 1.25953381457e-05\n",
      "\n",
      "Log step 86\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99972\n",
      "der alpha               -1.25953e-05\n",
      "step size                1.00783\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.001179480962485729\n",
      "Inverting matrix with precision 0.0005770486267030133\n",
      "increased step size\n",
      "it 87, g: 14716.8961613, incr: -3.86879037251e-07, sum lambda -9.99971092078, epsilon: 5.224783816588927e-08, L: 0.942614748153, norm grad_lambda: 1.25956140421e-05\n",
      "\n",
      "Log step 87\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99971\n",
      "der alpha               -1.25956e-05\n",
      "step size                1.00783\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0002901046628594633\n",
      "Inverting matrix with precision 0.0005193437768388865\n",
      "it 88, g: 14716.896162, incr: 6.85209670337e-07, sum lambda -9.99969755818, epsilon: 4.702305434930035e-08, L: 0.942614748153, norm grad_lambda: 1.25957841544e-05\n",
      "\n",
      "Log step 88\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.9997\n",
      "der alpha               -1.25958e-05\n",
      "step size                1.06088\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0013063672901104065\n",
      "Inverting matrix with precision 0.00046740938638501324\n",
      "increased step size\n",
      "it 89, g: 14716.8961617, incr: -3.28253008774e-07, sum lambda -9.99968419538, epsilon: 4.232074891437031e-08, L: 0.895484010746, norm grad_lambda: 1.2595969052e-05\n",
      "\n",
      "Log step 89\n",
      "-------------------  --------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99968\n",
      "der alpha               -1.2596e-05\n",
      "step size                1.06088\n",
      "-------------------  --------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 2, inner objective 13617.8411484, grad norm 0.0002784903212646676\n",
      "Inverting matrix with precision 0.0004206684591455989\n",
      "it 90, g: 14716.8961624, incr: 7.67493474996e-07, sum lambda -9.99967012908, epsilon: 3.808867402293328e-08, L: 0.895484010746, norm grad_lambda: 1.25961468399e-05\n",
      "\n",
      "Log step 90\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99967\n",
      "der alpha               -1.25961e-05\n",
      "step size                1.11671\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.00136760256016447\n",
      "Inverting matrix with precision 0.0003786016024648161\n",
      "increased step size\n",
      "it 91, g: 14716.8961621, incr: -3.1576200854e-07, sum lambda -9.9996560584, epsilon: 3.427980662063995e-08, L: 0.850709810208, norm grad_lambda: 1.26000714937e-05\n",
      "\n",
      "Log step 91\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99966\n",
      "der alpha               -1.26001e-05\n",
      "step size                1.11671\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0009428740577579497\n",
      "Inverting matrix with precision 0.0003407414452371062\n",
      "it 92, g: 14716.8961623, incr: 2.43495378527e-07, sum lambda -9.99964124695, epsilon: 3.0851825958575954e-08, L: 0.850709810208, norm grad_lambda: 1.26002461466e-05\n",
      "\n",
      "Log step 92\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99964\n",
      "der alpha               -1.26002e-05\n",
      "step size                1.17549\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0008092893552759649\n",
      "Inverting matrix with precision 0.00030666730158506437\n",
      "it 93, g: 14716.8961624, incr: 7.89532350609e-08, sum lambda -9.99962643528, epsilon: 2.776664336271836e-08, L: 0.850709810208, norm grad_lambda: 1.26004324403e-05\n",
      "\n",
      "Log step 93\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99963\n",
      "der alpha               -1.26004e-05\n",
      "step size                1.17549\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0006834901271353126\n",
      "Inverting matrix with precision 0.00027600057217975316\n",
      "it 94, g: 14716.8961625, incr: 7.62483978178e-08, sum lambda -9.99961162339, epsilon: 2.4989979026446523e-08, L: 0.850709810208, norm grad_lambda: 1.26006171909e-05\n",
      "\n",
      "Log step 94\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99961\n",
      "der alpha               -1.26006e-05\n",
      "step size                1.17549\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 2, inner objective 13617.8411484, grad norm 0.0002848956336193571\n",
      "Inverting matrix with precision 0.00024840051869196223\n",
      "it 95, g: 14716.8961629, incr: 4.42265445599e-07, sum lambda -9.99959681129, epsilon: 2.2490981123801872e-08, L: 0.850709810208, norm grad_lambda: 1.2600803472e-05\n",
      "\n",
      "Log step 95\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.9996\n",
      "der alpha               -1.26008e-05\n",
      "step size                1.17549\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0002848951181863637\n",
      "Inverting matrix with precision 0.00022356046682276003\n",
      "increased step size\n",
      "it 96, g: 14716.8961629, incr: -3.63797880709e-12, sum lambda -9.99958199896, epsilon: 2.0241883011421686e-08, L: 0.808174319698, norm grad_lambda: 1.26009884971e-05\n",
      "\n",
      "Log step 96\n",
      "-------------------  --------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99958\n",
      "der alpha               -1.2601e-05\n",
      "step size                1.17549\n",
      "-------------------  --------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.0002848950086414498\n",
      "Inverting matrix with precision 0.00020120442014048322\n",
      "increased step size\n",
      "it 97, g: 14716.8961629, incr: 0.0, sum lambda -9.99956640681, epsilon: 1.8217694710279517e-08, L: 0.767765603713, norm grad_lambda: 1.26011748451e-05\n",
      "\n",
      "Log step 97\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99957\n",
      "der alpha               -1.26012e-05\n",
      "step size                1.23736\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.00028489482126608606\n",
      "Inverting matrix with precision 0.00018108397812643332\n",
      "increased step size\n",
      "it 98, g: 14716.8961629, incr: 0.0, sum lambda -9.99954999377, epsilon: 1.6395925239251566e-08, L: 0.729377323527, norm grad_lambda: 1.2601369959e-05\n",
      "\n",
      "Log step 98\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99955\n",
      "der alpha               -1.26014e-05\n",
      "step size                1.30248\n",
      "-------------------  ---------------\n",
      "LBFGS decided finish!\n",
      "b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH            \\x00'\n",
      "inner level iterations: 1, inner objective 13617.8411484, grad norm 0.000280945345574433\n",
      "Inverting matrix with precision 0.00016297558027865888\n",
      "increased step size\n",
      "it 99, g: 14716.8961629, incr: -4.21459844802e-09, sum lambda -9.9995327166, epsilon: 1.475633271532641e-08, L: 0.692908457351, norm grad_lambda: 1.26015765678e-05\n",
      "\n",
      "Log step 99\n",
      "-------------------  ---------------\n",
      "training error       13616.1\n",
      "validation error     14716.9\n",
      "test error           14737.3\n",
      "validation accuracy      0.7727\n",
      "test accuracy            0.776033\n",
      "alpha                   -9.99953\n",
      "der alpha               -1.26016e-05\n",
      "step size                1.37103\n",
      "-------------------  ---------------\n"
     ]
    }
   ],
   "source": [
    "proj = lambda x: np.minimum(np.maximum(x, -10.), 10.)\n",
    "clf2, res2 = mlx.hoag_fit(dataset, alpha0=0., projection=proj, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAHoCAYAAABO0/lTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ecd8be518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=3, sharex=True)\n",
    "ax[0].plot(res2['alpha'][1:])\n",
    "ax[0].set_ylim(-11, 11)\n",
    "ax[0].set_title('HOAG alpha evolution during hyper-iterations (alpha in [-10, 10])')\n",
    "ax[1].plot(res2['der alpha'])\n",
    "ax[1].set_title('Derivative of alpha')\n",
    "\n",
    "ax[2].set_ylim(14500, 15000)\n",
    "ax[2].plot(res2['validation error'][1:])\n",
    "ax[2].set_title('Validation error');  # the one we're optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strange thing here is that the derivative w.r.t. $\\alpha$ remains quite negative even when it is 10... Could this be changed/avoided with a different $\\epsilon$ devrease strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
